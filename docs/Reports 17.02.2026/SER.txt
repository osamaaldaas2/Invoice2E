report 3: 
 # SRE RELIABILITY AUDIT — Invoice2E.1
 ## Site Reliability Engineer — Embedded Auditor Report

 **Date:** 2026-02-17
 **Scope:** Background workers, retry policies, credit atomicity, extraction loops, observability, performance boundaries
 **Methodology:** Direct source code inspection of every reliability-critical path

 ---

 ## 1. RETRY POLICIES AUDIT

 ### 1.1 Extraction-Level Retries (AI Math Validation)

 **Location:** `services/ai/gemini.extractor.ts:59-76`, `services/ai/mistral.extractor.ts:60-77`

 | Parameter | Value | Source |
 |-----------|-------|--------|
 | Max retries | 2 | `EXTRACTION_MAX_RETRIES` in `lib/constants.ts:106` |
 | Backoff | NONE (immediate) | Retry happens synchronously, no delay |
 | Scope | Per-extraction | Each `extractFromFile()` call has its own retry loop |
 | Convergence | Monotonic | Each retry sends previous JSON + validation errors |
 | Guard | `shouldRetry(attempt)` | `attempt < EXTRACTION_MAX_RETRIES` — bounded |

 **VERDICT: BOUNDED ✅** — Maximum 3 AI calls per extraction (1 initial + 2 retries).

 **RISK: MEDIUM — No backoff between retries.** If the AI provider is overloaded, 3 rapid requests can worsen the situation. The
 extraction retry loop is *inside* the batch retry loop (see 1.2), creating a retry amplification:

 ```
 Batch retry (3 attempts × backoff)
   → Extraction retry (3 attempts × no backoff)
   = Up to 9 AI calls per file in worst case
 ```

 ### 1.2 Batch-Level Retries (429/503 Transient Errors)

 **Location:** `services/batch/batch.processor.ts:18-68`

 | Parameter | Value | Source |
 |-----------|-------|--------|
 | Max retries | 3 | `BATCH_EXTRACTION.MAX_RETRIES` in `lib/constants.ts:85` |
 | Initial backoff | 5,000ms | `BATCH_EXTRACTION.INITIAL_BACKOFF_MS` |
 | Multiplier | 2× | `BATCH_EXTRACTION.BACKOFF_MULTIPLIER` |
 | Max backoff | 60,000ms | `BATCH_EXTRACTION.MAX_BACKOFF_MS` |
 | Schedule | 5s → 10s → 20s (capped at 60s) | Exponential with cap |
 | Retryable errors | 429, rate limit, quota, 503, overloaded, PARSE_ERROR | String matching on error message |

 **VERDICT: WELL-BOUNDED ✅** — Exponential backoff with cap. No infinite loop possible.

 **RISK: LOW** — The only concern is the string-based retry classification (`errorMessage.includes('429')`). If an error message like
 `"Invoice 429 Main Street failed"` occurs, it would be falsely classified as retryable. However, this is a low-probability edge case with
  bounded impact (3 extra retries max).

 ### 1.3 Stuck Job Recovery

 **Location:** `services/batch/batch.service.ts:534-609`

 | Scenario | Action | Threshold |
 |----------|--------|-----------|
 | Job stuck < 5min | Reset to `pending` | `thresholdMs = 300000` |
 | Job stuck > 15min (3×threshold) | Mark as `failed` | `ageMs > thresholdMs * 3` |
 | All files processed, status stuck | Finalize as `completed` | `completedFiles + failedFiles >= totalFiles` |

 **VERDICT: SOLID ✅** — Three-tier recovery: finalize, retry, or fail.

 **FINDING R-1 (MEDIUM): Reset-to-pending creates credit double-deduction risk.**
 When a stuck job is reset to `pending` (line 597), the worker picks it up again and calls `processBatch()`, which calls
 `creditsDbService.deductCredits()` at line 352. Credits were already deducted on the first run. The second run deducts again.

 The `allProcessed` guard (line 561) prevents this for completed jobs, but for partially-processed jobs (some files done, some pending),
 the reset re-deducts for ALL files including already-processed ones.

 ---

 ## 2. EXTRACTION QUALITY CONTROL AUDIT

 ### 2.1 Math Reconciliation

 **Location:** `lib/extraction-validator.ts:26-114`

 | Check | Formula | Tolerance | Source |
 |-------|---------|-----------|--------|
 | Line item total | `unitPrice × quantity ≈ totalPrice` | ±0.02 | `EXTRACTION_VALIDATION_TOLERANCE.LINE_ITEM` |
 | Subtotal | `Σ(lineItems.totalPrice) ≈ subtotal` | ±0.10 | `.SUBTOTAL` |
 | Tax amount | `subtotal × taxRate / 100 ≈ taxAmount` | ±0.05 | `.TAX` |
 | Grand total | `subtotal + taxAmount ≈ totalAmount` | ±0.05 | `.TOTAL` |

 **VERDICT: DETERMINISTIC ✅** — Uses `approxEqual(a, b, tolerance)` with fixed tolerances.

 **FINDING R-2 (MEDIUM): Tolerance masks real extraction errors for small invoices.**
 For an invoice with `subtotal = 1.00`, the ±0.10 tolerance means any subtotal between 0.90 and 1.10 passes validation. That's a ±10%
 error margin. For a `totalAmount = 0.50` invoice, the ±0.05 tolerance allows 10% drift. This is problematic for micro-invoices common in
 SaaS and subscription billing.

 **FINDING R-3 (LOW): Tax validation doesn't handle mixed-rate invoices.**
 The validator checks `taxAmount ≈ subtotal × taxRate / 100` using a single document-level `taxRate`. Invoices with mixed VAT rates (e.g.,
  19% on some items, 7% on others) will always fail this check. The AI retry loop will fire 2 retries, fail, and tag warnings — wasting AI
  API calls.

 ### 2.2 Warning Propagation

 **Location:** `services/ai/gemini.extractor.ts:80-82`, `services/ai/mistral.extractor.ts:80-82`

 When retries are exhausted and validation still fails:
 1. Confidence is degraded: `Math.max(0.3, confidence - 0.2)`
 2. Warnings are serialized as strings: `"${field}: ${message}"`
 3. Stored in `data.validationWarnings` (string array)
 4. Persisted to DB inside `extractionData` JSONB column
 5. Surfaced in frontend on the convert page

 **FINDING R-4 (MEDIUM): Warnings lose structured metadata.**
 The `validationWarnings` field serializes `ExtractionValidationError` objects to strings, losing `expected`, `actual`, and `field` as
 discrete values. Downstream consumers (batch-validate, convert page) cannot filter or sort by severity. The batch-validate route
 (correctly) re-validates with structured output, but the extraction warnings in the DB are unstructured.

 ---

 ## 3. BACKGROUND JOB SAFETY AUDIT

 ### 3.1 Job Claim (Duplicate Prevention)

 **Location:** `services/batch/batch.service.ts:640-660`

 ```sql
 UPDATE batch_jobs
 SET status = 'processing', processing_started_at = NOW()
 WHERE id = $1 AND status = 'pending'
 RETURNING id, user_id, input_file_path, source_type, output_format
 ```

 **VERDICT: SAFE ✅** — Optimistic lock via `WHERE status = 'pending'`. If two workers race, only one gets the `RETURNING` row. The loser
 gets `null` and returns `false`.

 ### 3.2 Credit Deduction Atomicity

 **Location:** `db/migrations/008_atomic_credit_operations.sql:50-104`

 The `safe_deduct_credits` PostgreSQL function:
 1. Acquires `FOR UPDATE` row lock on `user_credits`
 2. Checks `available_credits >= p_amount`
 3. Atomically decrements and increments `used_credits`
 4. Logs to `audit_logs`
 5. All within one transaction

 **VERDICT: SOLID ✅** — Row-level lock prevents double-spend.

 ### 3.3 Credit Addition (Payment Webhook) Idempotency

 **Location:** `db/migrations/026_atomic_verify_credits.sql`

 The `verify_and_add_credits` function:
 1. Inserts into `webhook_events` (UNIQUE on `event_id + provider`)
 2. If unique violation → returns `already_processed: true`
 3. Otherwise → atomically adds credits
 4. All within one transaction

 **VERDICT: EXCELLENT ✅** — Idempotency-by-design. Duplicate webhooks are safely rejected.

 ### 3.4 Batch Credit Flow — The Full Picture

 **FINDING R-5 (HIGH): Credit leak on stuck-job recovery with partial progress.**

 The batch credit lifecycle:

 ```
 1. processBatch() deducts N credits upfront (line 352)
 2. Files process concurrently
 3. If job crashes mid-processing → status remains 'processing'
 4. recoverStuckJobs() resets to 'pending' (line 597)
 5. Worker picks up job again → processBatch() deducts N credits AGAIN
 6. Double deduction for already-processed files
 ```

 The recovery code at line 561-579 checks `allProcessed` to finalize completed jobs, but this only works when ALL files are done. For
 partial crashes (e.g., 3 of 5 files done, then serverless timeout), the job resets to pending and re-processes everything.

 **Impact:** User loses `N` credits (entire batch size) on each recovery cycle.

 **Mitigation path:**
 - Before deducting in `processBatch()`, check if credits were already deducted:
   ```sql
   SELECT * FROM audit_logs
   WHERE action = 'credits_deducted'
   AND changes->>'reason' = 'batch:deduct:{jobId}'
   ```
 - Or: store `credits_deducted: true` flag on the batch_jobs row.

 ### 3.5 Multi-Invoice Credit Flow (Inline ≤3 Invoices)

 **Location:** `app/api/invoices/extract/route.ts:144-272`

 1. Credits deducted upfront: `deductCredits(userId, requiredCredits, 'extraction:deduct:multi')` (line 147)
 2. Each segment extracted in parallel chunks of 5
 3. Failed segments refunded: `addCredits(userId, failCount, 'extraction:refund:multi')` (line 259)

 **FINDING R-6 (MEDIUM): Refund without idempotency key.**
 The `addCredits()` call at line 259 does NOT pass a `referenceId`. If the HTTP request is retried (e.g., Vercel timeout → client
 auto-retry), credits are deducted again (new request = new `deductCredits` call), but the refund from the first request may also
 complete. This creates a race:

 | Scenario | Credits Flow |
 |----------|-------------|
 | Normal | -5 deducted, -2 failed = -3 net (correct) |
 | Retry storm | -5 deducted (R1), -5 deducted (R2), +2 refund (R1), +2 refund (R2) = -6 net (wrong, should be -3) |

 ### 3.6 Refund Correctness

 **Location:** `services/batch/batch.processor.ts:432-447`

 ```typescript
 if (failCount > 0) {
   await creditsDbService.addCredits(userId, failCount, `batch:refund:${jobId}`, jobId);
 }
 ```

 **VERDICT: PARTIALLY SAFE** — The `jobId` is passed as `referenceId` for audit, but `addCredits()` does NOT enforce uniqueness on
 `referenceId`. A second call with the same `batch:refund:{jobId}` source will happily add credits again.

 ### 3.7 Failure Recovery Summary

 | Failure Mode | Recovery | Credit Safety |
 |-------------|----------|--------------|
 | AI 429/503 | Exponential backoff retry (3 attempts) | Safe — retry is pre-deduction |
 | AI extraction crash | Refund via `addCredits` | Mostly safe (no idempotency) |
 | Batch job serverless timeout | `recoverStuckJobs()` resets to pending | **UNSAFE — double deduction** |
 | All files processed, status stuck | Finalize directly | Safe |
 | Multi-invoice inline retry | Client retries entire request | **UNSAFE — double deduction** |
 | Payment webhook duplicate | `verify_and_add_credits` UNIQUE constraint | **Safe — idempotent** |
 | Admin refund race condition | No lock between check and update | UNSAFE (see CISO report H-1) |

 ---

 ## 4. DATA AUTHORITY AUDIT

 ### 4.1 SessionStorage vs DB

 **Location:** `app/convert/[extractionId]/page.tsx`, `components/forms/invoice-review/useInvoiceReviewForm.ts`

 **Data Flow:**
 ```
 Review form → auto-save to sessionStorage (every 30s)
 Submit review → persist to DB (invoice_extractions.extraction_data)
 Navigate to /convert → READS FROM SESSIONSTORAGE FIRST, DB fallback
 Convert API called → client sends invoiceData from sessionStorage
 ```

 **FINDING R-7 (HIGH): Convert page trusts client-provided data over DB.**
 The `/api/invoices/convert` route accepts `invoiceData` from the POST body (client-provided). It does NOT fetch the extraction from DB to
  verify. The ownership check at line 265 verifies the `conversion` record belongs to the user, but the `invoiceData` is whatever the
 client sent.

 This means:
 1. Stale sessionStorage data → stale conversion
 2. Tampered sessionStorage → tampered conversion
 3. Page refresh clears sessionStorage → DB fallback (different data possible)
 4. Two tabs open → different sessionStorage states → unpredictable conversion

 ### 4.2 Concurrent Review Race

 **Location:** `app/api/invoices/review/route.ts`

 No optimistic locking. Two concurrent PATCH requests both read `extraction_data`, both write different merged results. Last-write-wins
 with no conflict detection and no log event.

 ### 4.3 Batch Apply Race

 **Location:** `app/api/invoices/batch-apply/route.ts`

 Same issue — batch field application (`sellerName`, `buyerReference`) writes to `extraction_data` without version check. If a user
 applies batch fields while another tab submits a review, fields are silently lost.

 ---

 ## 5. OBSERVABILITY AUDIT

 ### 5.1 Request ID Propagation

 | Component | Has requestId? | Source |
 |-----------|---------------|--------|
 | `middleware.ts` | ✅ Generates UUID | `crypto.randomUUID()` line 7 |
 | Request headers | ✅ Set | `requestHeaders.set('x-request-id', requestId)` line 36 |
 | Response headers | ✅ Set | `response.headers.set('x-request-id', requestId)` line 40 |
 | API route handlers | ❌ NOT extracted | No handler calls `request.headers.get('x-request-id')` |
 | Logger context | ❌ NOT wrapped | `logContext.run()` is never called in any API route |
 | Error responses | ❌ NOT included | `handleApiError()` does not include requestId |

 **FINDING R-8 (HIGH): RequestId is generated but NEVER USED.** The entire `logContext` / `AsyncLocalStorage` infrastructure in
 `lib/logger.ts` is dead code in production. No API handler wraps execution with `logContext.run({ requestId })`. All log entries lack
 correlation ID.

 ### 5.2 Validation Failure Logging

 | Route | Logs ruleId? | Structured? |
 |-------|-------------|------------|
 | `batch-validate/route.ts` | ✅ `[${e.ruleId}]` prefix | Partially (string format) |
 | `convert/route.ts` | ✅ Extracts `ruleIds` array | ✅ Structured in logger |
 | `gemini.extractor.ts` | ❌ Logs error objects only | Array of `{field, message}` |
 | `mistral.extractor.ts` | ❌ Same as above | Same |

 ### 5.3 Missing Metrics

 The following operational metrics are **NOT tracked anywhere** in the codebase:

 | Metric | Status | Impact |
 |--------|--------|--------|
 | Extraction failure rate per provider | ❌ Missing | Cannot compare Gemini vs Mistral reliability |
 | Retry rate per extraction | ❌ Missing | Cannot detect AI model quality degradation |
 | Retry success rate (retried → passed validation) | ❌ Missing | Cannot measure retry ROI |
 | Validator rejection rate per format | ❌ Missing | Cannot identify problematic formats |
 | Credit balance mismatches | ❌ Missing | Cannot detect deduction bugs |
 | Batch job duration P50/P95/P99 | ❌ Missing | Cannot set SLOs |
 | Worker queue depth (pending jobs) | ❌ Missing | Cannot scale workers |
 | Multi-invoice expansion ratio | ❌ Missing | Cannot predict credit consumption |
 | PDF split success rate | ❌ Missing | Cannot detect splitting regressions |

 ### 5.4 Error Response Quality

 **Location:** `lib/api-helpers.ts:36-70`

 | Error Type | Status Code | Client Message | Internal Logged? |
 |-----------|-------------|---------------|-----------------|
 | ZodError | 400 | First error message | ✅ |
 | UnauthorizedError | 401 | Error message | ✅ |
 | ForbiddenError | 403 | Error message | ✅ |
 | NotFoundError | 404 | Error message | ✅ |
 | AppError | Custom | Error message | ✅ |
 | Unknown Error | 500 | "Internal server error" | ✅ |

 **FINDING R-9 (MEDIUM): No requestId in error responses.** When a user reports "I got an error", support cannot locate the corresponding
 server log. The response body contains only `{ success: false, error: "..." }` — no trace ID, no timestamp.

 ---

 ## 6. PERFORMANCE RISKS AUDIT

 ### 6.1 Large Multi-Invoice PDFs

 **Location:** `app/api/invoices/extract/route.ts`

 | Threshold | Action | Risk |
 |-----------|--------|------|
 | ≤3 invoices | Inline parallel extraction | LOW — bounded concurrency (MULTI_INVOICE_CONCURRENCY=5) |
 | >3 invoices | Background job | LOW — worker processes chunks of 5 |
 | Boundary detection | 50 pages max | `MAX_PAGES_FOR_BOUNDARY_DETECTION=50` in constants |

 **FINDING R-10 (MEDIUM): No memory limit on PDF buffer duplication.**
 When splitting a 50-page, 25MB PDF into segments:
 1. `buffer` (25MB) loaded into memory (line 130)
 2. `splitByPageGroups()` creates N sub-PDFs in memory simultaneously
 3. Each sub-PDF kept in `splitBuffers[]` array
 4. All N buffers held simultaneously during parallel extraction

 For a 25MB PDF with 10 invoices at 5 pages each:
 - Base: 25MB
 - Split buffers: ~2.5MB × 10 = 25MB
 - AI calls hold base64-encoded buffers: ~33MB × 5 (concurrency) = 165MB
 - **Total peak: ~215MB per request**

 On Vercel (1024MB default), this leaves ~800MB for other concurrent requests. With 3 concurrent multi-invoice extractions, memory
 pressure reaches ~645MB — approaching the limit.

 ### 6.2 Worker Concurrency

 **Location:** `app/api/internal/batch-worker/route.ts`

 ```typescript
 const maxJobs = Math.min(maxJobsParam, 20); // line 60 — capped at 20
 for (let i = 0; i < maxJobs; i++) {
     const handled = await batchService.runWorkerOnce(); // Sequential
     if (!handled) break;
 }
 ```

 **VERDICT: SAFE ✅** — Jobs processed sequentially within a single worker invocation. Cap of 20 jobs per call. Each job processes files
 in chunks of 5 concurrently.

 **FINDING R-11 (LOW): No worker concurrency guard across instances.**
 Multiple `/api/internal/batch-worker` calls can run simultaneously (separate serverless instances). Each claims jobs independently via
 the optimistic lock (`WHERE status = 'pending'`), so they don't process the same job. However, 10 simultaneous worker calls = 10 × 5 = 50
  concurrent AI API calls, which may exceed provider rate limits.

 ### 6.3 PDF Splitting Safety

 **Location:** `services/pdf-splitter.service.ts`

 ```typescript
 const sourcePdf = await PDFDocument.load(new Uint8Array(sourceBuffer), { ignoreEncryption: true });
 ```

 **FINDING R-12 (LOW): `ignoreEncryption: true` bypasses PDF security.**
 Encrypted PDFs with restricted permissions (no-copy, no-print) are loaded and split without honoring access controls. This is intentional
  for extraction but could be a compliance concern if the PDF contains DRM-protected content.

 ### 6.4 ZIP Bomb Protection

 **Location:** `services/batch/batch.service.ts:84-131`

 ```typescript
 if (zipBuffer.length > this.MAX_TOTAL_SIZE) { throw ... } // Line 85: 500MB
 if (content.length > this.MAX_FILE_SIZE) { throw ... }    // Line 109: 25MB per file
 totalExtractedSize += content.length;
 if (totalExtractedSize > this.MAX_TOTAL_SIZE) { throw ... } // Line 116: cumulative
 if (files.length >= this.MAX_FILES) { throw ... }           // Line 104: 100 files
 ```

 **VERDICT: SOLID ✅** — Four-layer defense: compressed size, per-file size, cumulative extracted size, file count.

 ---

 ## RELIABILITY RISK TABLE

 | ID | Severity | Component | Failure Mode | Current Mitigation | Residual Risk |
 |----|----------|-----------|-------------|-------------------|---------------|
 | **R-1** | HIGH | `batch.service.ts:597` | Stuck job recovery re-deducts credits | `allProcessed` guard for completed jobs |
 Partial-progress jobs lose credits |
 | **R-5** | HIGH | `batch.processor.ts:352` | Double credit deduction on job recovery | None | Full batch cost doubled |
 | **R-7** | HIGH | `convert/page.tsx` | SessionStorage stale data → wrong conversion | DB fallback (only on empty sessionStorage) | Stale
  data preferred over fresh DB |
 | **R-8** | HIGH | `logger.ts` / all routes | RequestId generated but never propagated | None — dead code | Zero request traceability |
 | **R-2** | MEDIUM | `extraction-validator.ts` | ±10% tolerance on micro-invoices | Fixed tolerance values | Silent rounding errors |
 | **R-4** | MEDIUM | `gemini.extractor.ts:80-82` | Validation warnings serialized to strings | Frontend displays strings | No structured
 filtering |
 | **R-6** | MEDIUM | `extract/route.ts:259` | Credit refund without idempotency key | None | Double refund on HTTP retry |
 | **R-9** | MEDIUM | `api-helpers.ts` | No requestId in error responses | None | Support cannot trace errors |
 | **R-10** | MEDIUM | `extract/route.ts` | ~215MB peak memory per multi-invoice request | Serverless memory limits | OOM on concurrent
 requests |
 | **R-3** | LOW | `extraction-validator.ts` | Mixed tax rates always fail validation | Retry loop (wastes API calls) | 2 unnecessary AI
 calls |
 | **R-11** | LOW | `batch-worker/route.ts` | Multiple workers exceed AI rate limits | Per-adapter token bucket | Token bucket is
 per-instance |
 | **R-12** | LOW | `pdf-splitter.service.ts` | Encrypted PDFs processed silently | None | Compliance concern |

 ---

 ## FAILURE MODE SCENARIOS

 ### Scenario 1: "The Silent Credit Drain"
 **Trigger:** Vercel serverless timeout after 300s during batch processing (5 of 10 files done)
 **Chain:**
 1. Worker processes 5 files, serverless instance killed at 5min mark
 2. Batch job remains `status = 'processing'`
 3. `recoverStuckJobs()` runs after 5 minutes → resets to `pending`
 4. New worker claims job → `processBatch()` deducts 10 credits AGAIN
 5. User now charged 20 credits for 10 files
 6. 5 already-completed extractions are duplicated (orphaned DB records)
 7. Credits refunded for any failures, but deduction is irreversible for successes

 **Probability:** MEDIUM (serverless timeouts are common under load)
 **Impact:** Financial — user loses credits
 **Detection:** No automated monitoring; requires manual audit log query

 ### Scenario 2: "The Stale Invoice"
 **Trigger:** User opens two tabs, edits in tab A, converts in tab B
 **Chain:**
 1. Tab A: User reviews invoice, changes VAT ID → saved to sessionStorage
 2. Tab B: User navigates to /convert for same extractionId
 3. Tab B loads sessionStorage (which has tab A's CURRENT state — shared!)
 4. User submits review in tab A → DB updated
 5. Tab B's sessionStorage still has pre-review data from initial auto-save
 6. User refreshes tab B → sessionStorage cleared → DB fallback (correct data)
 7. BUT if user hits "Convert" before refresh → stale data used

 **Probability:** LOW-MEDIUM (common in multi-tab workflows)
 **Impact:** Compliance — wrong invoice generated
 **Detection:** None

 ### Scenario 3: "The Retry Amplifier"
 **Trigger:** Gemini API returns 503 during batch of 100 invoices
 **Chain:**
 1. Batch processor fires 5 concurrent extractions
 2. Each extraction hits 503 → classified as retryable
 3. Each retries 3× with backoff (5s, 10s, 20s) = 35 seconds per file
 4. But INSIDE each retry, the extractor also retries 2× for math validation
 5. Worst case: 3 batch retries × 3 validation retries = 9 API calls per file
 6. 100 files × 9 calls = 900 API calls (intended: 100)
 7. This amplification only fires if 503 is transient (succeeds on retry)

 **Probability:** LOW (requires transient 503 + validation failure simultaneously)
 **Impact:** API quota drain, cost
 **Detection:** No retry-count metric logged

 ### Scenario 4: "The Orphaned Background Job"
 **Trigger:** `triggerBatchWorker()` fires but worker URL is wrong (DNS failure)
 **Chain:**
 1. Multi-invoice PDF (>3 invoices) detected → background job created
 2. Credits deducted upfront
 3. `triggerBatchWorker()` uses `AbortController` with 3s timeout
 4. Request fails (DNS, network) → error caught and swallowed (line 37)
 5. Job sits in `pending` status forever
 6. No cron/scheduler to pick up orphaned jobs
 7. User sees "Processing..." indefinitely

 **Probability:** LOW (requires network failure to own API)
 **Mitigation:** `extract/route.ts` lines 403-420 implement a polling-based recovery — the GET handler checks for stuck pending jobs and
 re-triggers the worker. This is a safety net, but it requires the frontend to keep polling.

 ---

 ## OBSERVABILITY GAPS

 | Gap | Impact | Priority |
 |-----|--------|----------|
 | **requestId dead code** — generated in middleware, never consumed by handlers or logger | Cannot correlate any request across log
 entries | P0 |
 | **No extraction metrics** — success rate, retry rate, provider comparison | Cannot detect AI model quality degradation | P1 |
 | **No credit balance reconciliation** — no periodic check for deduct/refund mismatches | Silent credit leaks undetected | P1 |
 | **No worker queue depth metric** — pending job count not tracked | Cannot set auto-scaling rules | P1 |
 | **No batch job duration tracking** — no P50/P95/P99 metrics | Cannot set SLOs or detect regressions | P2 |
 | **Error responses lack trace ID** — clients can't reference specific failures | Support ticket resolution time increases | P2 |
 | **Validation warnings unstructured** — string format loses field/expected/actual | Cannot analyze extraction quality patterns | P2 |
 | **No memory usage logging** — no tracking of peak memory during PDF operations | OOM crashes are invisible until they happen | P3 |

 ---

 ## IMMEDIATE SRE HOTFIX RECOMMENDATIONS

 ### HOTFIX 1: Prevent Credit Double-Deduction on Job Recovery (R-1, R-5)

 **File:** `services/batch/batch.processor.ts:350-356`

 **Fix:** Add a `credits_deducted` flag to the `batch_jobs` row. Check before deducting:

 ```typescript
 // Before deducting, check if already deducted for this job
 const { data: jobRow } = await supabase
   .from('batch_jobs')
   .select('credits_deducted')
   .eq('id', jobId)
   .single();

 if (!jobRow?.credits_deducted) {
   const creditsDeducted = await creditsDbService.deductCredits(userId, totalFiles, `batch:deduct:${jobId}`);
   if (creditsDeducted) {
     await supabase.from('batch_jobs').update({ credits_deducted: true }).eq('id', jobId);
   }
 }
 ```

 **Migration:** `ALTER TABLE batch_jobs ADD COLUMN credits_deducted BOOLEAN DEFAULT FALSE;`

 ### HOTFIX 2: Wire Up RequestId Propagation (R-8)

 **File:** Every API route handler

 **Fix:** Extract requestId from headers and wrap handler in logContext:

 ```typescript
 export async function POST(request: NextRequest) {
   const requestId = request.headers.get('x-request-id') || crypto.randomUUID();
   return logContext.run({ requestId }, async () => {
     // ... existing handler code
   });
 }
 ```

 ### HOTFIX 3: Convert Page Must Fetch from DB (R-7)

 **File:** `app/convert/[extractionId]/page.tsx`

 **Fix:** Always fetch extraction from DB before conversion. Use sessionStorage only as a UI cache hint, not as the conversion data
 source.

 ---

 ## LONG-TERM STABILITY IMPROVEMENTS

 | Priority | Improvement | Effort | Impact |
 |----------|-------------|--------|--------|
 | P0 | Add `credits_deducted` flag to batch_jobs to prevent double-deduction | 2h | Eliminates credit leak |
 | P0 | Wire requestId through all API handlers | 4h | Enables production debugging |
 | P1 | Add structured metrics logging (extraction rate, retry rate, credit balance) | 8h | Enables SLO monitoring |
 | P1 | Add optimistic locking to extraction updates (review endpoint) | 4h | Prevents data loss on concurrent edits |
 | P1 | Add idempotency key to batch credit refunds | 2h | Prevents double-refund on HTTP retry |
 | P2 | Scale validation tolerances by invoice total (relative, not absolute) | 4h | Fixes micro-invoice accuracy |
 | P2 | Add requestId to all error API responses | 2h | Reduces support ticket resolution time |
 | P2 | Track per-provider extraction metrics | 8h | Enables AI provider comparison |
 | P3 | Add memory usage tracking for PDF operations | 4h | Prevents OOM surprise |
 | P3 | Add mixed-rate tax validation (per-line, not document-level) | 8h | Eliminates unnecessary retries |

 ---

 ## FINAL VERDICT

 ### SLA Risk Level: **MEDIUM**

 The system has **strong foundations** (atomic credit operations, bounded retries, optimistic job claiming) but contains **two reliability
  holes** that can cause real financial impact:

 1. **Credit double-deduction on job recovery** (R-1/R-5) — affects every stuck batch job
 2. **RequestId dead code** (R-8) — makes production incident response nearly impossible

 ### Biggest Production Failure Risk

 **Credit double-deduction on stuck batch job recovery.** When a serverless instance times out mid-batch (a normal occurrence under load),
  the recovery mechanism re-processes the entire batch and deducts credits a second time. The user has no visibility into this, the system
  has no automated detection, and the only recovery is manual credit adjustment by an admin. At scale (100-file batches × multiple users),
  this can drain user credit balances silently.

 **Estimated blast radius:** Every batch job that experiences a serverless timeout (estimated 2-5% of batch jobs under load) will
 double-deduct credits. For a 100-file batch at 1 credit/file, that's 100 credits lost per incident.
